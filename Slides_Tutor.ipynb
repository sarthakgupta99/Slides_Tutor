{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthakgupta99/Slides_Tutor/blob/main/Slides_Tutor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6s6AcPt08K5"
      },
      "outputs": [],
      "source": [
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install bitsandBytes\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GPUtil"
      ],
      "metadata": {
        "id": "v16t51CO_NR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import GPUtil\n",
        "import os\n",
        "\n",
        "GPUtil.showUtilization()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available, using CPU instead\")\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "HZv4rCgtAFuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\n",
        "from huggingface_hub import notebook_login  # for interactive login in notebooks\n",
        "from datasets import load_dataset\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "# âœ… Always just call this in a notebook/Colab environment\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "Ub5Kq-xbAyqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
      ],
      "metadata": {
        "id": "AytCd6RuBj5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files={\"train\": [\"/content/genetics.txt\"]},\n",
        "    split=\"train\",\n",
        "    encoding=\"latin-1\" # Added encoding parameter\n",
        ")"
      ],
      "metadata": {
        "id": "cNS9dNEhDPE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[\"text\"][1]"
      ],
      "metadata": {
        "id": "cDZFH7XOD2JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
      ],
      "metadata": {
        "id": "W_c3rMnbEfYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = []\n",
        "for phrase in train_dataset:\n",
        "  tokenized_train_dataset.append(tokenizer(phrase[\"text\"]))"
      ],
      "metadata": {
        "id": "T941FWBIG8DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset[1]"
      ],
      "metadata": {
        "id": "eqXip5GPHWQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset[2]"
      ],
      "metadata": {
        "id": "dFJzKBcVHX4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "id": "2Y1SDgStHpOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "fmW4KgZlHucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=\"./finetunedModel\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=1e-4,\n",
        "        max_steps=20,\n",
        "        bf16=False,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_dir=\"./log\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_steps=50,\n",
        "        logging_steps=10\n",
        "\n",
        "),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache=False\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "02gX--VxILA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, LlamaTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "nf4Config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config=nf4Config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        "  )\n"
      ],
      "metadata": {
        "id": "BMOLdfgmKQ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True\n",
        "\n",
        "                              )\n",
        "\n",
        "modelFinetuned = PeftModel.from_pretrained(base_model, \"finetunedModel/checkpoint-20\")"
      ],
      "metadata": {
        "id": "lOIhR2C2MISD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What is a gene?\"\n",
        "\n",
        "eval_prompt = f\"Question: {user_question} Just answer this question accurately and concisely.\\n\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens=1024)[0], skip_special_tokens=True))\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hsl5SpQrMblq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSP6lR1YNWnJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}